{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzFvHEo0BgnW"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import WhisperProcessor, WhisperModel\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UlOL29aBmYk"
      },
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"sanskarhim/data-audio\")\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by7pkhyVBsV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyhcqSPjB0bD"
      },
      "outputs": [],
      "source": [
        "audio_files = os.listdir(path + '/preprocessed_dataset')\n",
        "\n",
        "for files in audio_files:\n",
        "    print(files)\n",
        "print(len(audio_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pq3TcOeB83-"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    \"bengali\": 0, \"gujarati\": 1, \"hindi\": 2, \"kannada\": 3, \"malayalam\": 4,\n",
        "    \"marathi\": 5, \"urdu\": 6, \"tamil\": 7, \"telugu\": 8\n",
        "}\n",
        "id2label = {v: k for k, v in label_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvQfaS_qB_iT"
      },
      "outputs": [],
      "source": [
        "def get_filepaths_and_labels(data_dir):\n",
        "    filepaths, labels = [], []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith(\".mp3\") or filename.endswith(\".wav\"):\n",
        "            lang_prefix = filename.split(\"_\")[0].lower()\n",
        "            if lang_prefix in label_map:\n",
        "                filepaths.append(os.path.join(data_dir, filename))\n",
        "                labels.append(label_map[lang_prefix])\n",
        "    return filepaths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfa-yYWrCCiw"
      },
      "outputs": [],
      "source": [
        "class LIDDataset(Dataset):\n",
        "    def __init__(self, filepaths, labels, processor):\n",
        "        self.filepaths = filepaths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.resampler = T.Resample(orig_freq=48000, new_freq=16000)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sr = torchaudio.load(self.filepaths[idx])\n",
        "        if sr != 16000:\n",
        "            waveform = self.resampler(waveform)\n",
        "        inputs = self.processor(waveform[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "        return inputs.input_features.squeeze(0), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA3CYdZxCOKR"
      },
      "outputs": [],
      "source": [
        "class WhisperClassifier(nn.Module):\n",
        "    def __init__(self, whisper_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = whisper_model.encoder\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(whisper_model.config.d_model, num_classes)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = self.encoder(input_features=input_features)\n",
        "        x = encoder_outputs.last_hidden_state\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(x).squeeze(2)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ8UKZe-CRB5"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for features, labels in dataloader:\n",
        "        features = features.to(device)\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
        "        logits = model(features)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LJI6R2LCUH1"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features = features.to(device)\n",
        "            logits = model(features)\n",
        "            pred = torch.argmax(logits, dim=1).cpu()\n",
        "            preds.extend(pred.numpy())\n",
        "            targets.extend(labels)\n",
        "\n",
        "            labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "            correct += (pred == labels_tensor).sum().item()\n",
        "            total += labels_tensor.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return preds, targets, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcI8WnGdCW0f"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=id2label.values(), yticklabels=id2label.values())\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE6LJdQyCZm5"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_acc, val_acc):\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
        "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Train vs Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJR2IK3RCcBU"
      },
      "outputs": [],
      "source": [
        "def save_model(model, processor, model_path=\"whisper_classifier.pt\", processor_path=\"processor\"):\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    processor.save_pretrained(processor_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    print(f\"Processor saved to {processor_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_DnqpkICfmQ"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path, processor_path, base_model_name=\"openai/whisper-base\", num_classes=10, device=\"cpu\"):\n",
        "    processor = WhisperProcessor.from_pretrained(processor_path)\n",
        "    whisper_model = WhisperModel.from_pretrained(base_model_name)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION,\n",
        "        inference_mode=True,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "    )\n",
        "    whisper_model = get_peft_model(whisper_model, peft_config)\n",
        "\n",
        "    model = WhisperClassifier(whisper_model, num_classes=num_classes)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtq7v9BGCkSw"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    data_dir = \"/kaggle/input/data-audio/preprocessed_dataset\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    filepaths, labels = get_filepaths_and_labels(data_dir)\n",
        "\n",
        "    train_fp, temp_fp, train_lb, temp_lb = train_test_split(\n",
        "        filepaths, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "    val_fp, test_fp, val_lb, test_lb = train_test_split(\n",
        "        temp_fp, temp_lb, test_size=0.5, stratify=temp_lb, random_state=42)\n",
        "\n",
        "    model_name = \"openai/whisper-base\"\n",
        "    processor = WhisperProcessor.from_pretrained(model_name)\n",
        "    whisper_model = WhisperModel.from_pretrained(model_name)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "    )\n",
        "    whisper_model = get_peft_model(whisper_model, peft_config)\n",
        "\n",
        "    model = WhisperClassifier(whisper_model, num_classes=10).to(device)\n",
        "\n",
        "    train_ds = LIDDataset(train_fp, train_lb, processor)\n",
        "    val_ds = LIDDataset(val_fp, val_lb, processor)\n",
        "    test_ds = LIDDataset(test_fp, test_lb, processor)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "    val_dl = DataLoader(val_ds, batch_size=8)\n",
        "    test_dl = DataLoader(test_ds, batch_size=8)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(7):\n",
        "        train_loss, train_acc = train(model, train_dl, optimizer, criterion, device)\n",
        "        _, _, val_acc = evaluate(model, val_dl, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    y_pred, y_true, _ = evaluate(model, test_dl, device)\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "\n",
        "    labels_list = list(id2label.keys())\n",
        "    target_names_list = [id2label[i] for i in labels_list]\n",
        "    print(\"Classification Report:\\n\", classification_report(\n",
        "        y_true, y_pred,\n",
        "        labels=labels_list,\n",
        "        target_names=target_names_list,\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    plot_confusion_matrix(y_true, y_pred)\n",
        "    plot_accuracy(train_accuracies, val_accuracies)\n",
        "\n",
        "    # Save the trained model and processor\n",
        "    save_model(model, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "482J4EtOCnC1"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
